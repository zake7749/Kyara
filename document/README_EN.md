# Kyara: Knowledge Yielding Adaptive Retrieval Augmentation for LLM Fine-tuning

<p align="left">
    ğŸ¤— <a href="https://huggingface.co/zake7749/gemma-2-2b-it-chinese-kyara-dpo">Hugging Face</a>&nbsp ï½œ ğŸš€<a href="https://github.com/zake7749/kyara">Github</a>&nbsp ï½œ &nbspğŸ“‘ <a href="#">Paper</a>&nbsp ï½œ &nbspğŸ“– <a href="#">English</a>&nbsp | &nbspğŸ“– <a href="https://github.com/zake7749/kyara">Chinese</a>
</p>
<div style="text-align: center;">
  <img src="https://i.imgur.com/QiWlcYJ.jpeg" alt="kyara"/>
</div>

Kyara is an experimental strategy for fine-tuning language models, designed to effectively enhance the model's knowledge adaptation and language understanding capabilities through knowledge retrieval augmentation.

To validate the effectiveness of this method, We conducted full-parameter fine-tuning on  `Gemma-2-2b-it`, resulting in the first version of the Kyara model. Preliminary evaluation results can be seen in the [Benchmark](#benchmark) section.

## Table of Content

- [Benchmark](#benchmark)
   * [**General Benchmark**](#general-benchmark)
   * [**Alignment Benchmark**](#alignment-benchmark)
- [Feature](#feature)
   * [System Prompt](#system-prompt)
      + [Input](#input)
      + [Output](#output)
      + [Input](#input-1)
      + [Output](#output-1)
   * [Retrieval Augmented Generation (Experimental)](#retrieval-augmented-generation-experimental)
      + [Input](#input-2)
      + [Output](#output-2)
- [Method](#method)
   * [Dataset Summary](#dataset-summary)
   * [Dataset Construction](#dataset-construction)
      + [Base Dataset: Knowledge Injection with Retrieval Augmentation](#base-dataset-knowledge-injection-with-retrieval-augmentation)
         - [Chinese Math Dataset](#chinese-math-dataset)
      + [High Quality Dataset: Model Refinement ](#high-quality-dataset-model-refinement)
   * [Preference Learning](#preference-learning)
      + [Chinese DPO](#chinese-dpo)
         - [SPIN/SPPO](#spinsppo)
         - [RLAIF](#rlaif)

## Benchmark

### **General Benchmark**

All evaluations are based-on zero-shot.

| Metric                   | Kyara-2b-it    | Gemma-2-2b-it |
|--------------------------|----------|-------------|
| **[TMMLUPlus](https://huggingface.co/datasets/ikala/tmmluplus)**            | **39.22** | 36.73    |
| &emsp;- STEM               | **40.86**   | 37.84      |
| &emsp;- Humanities         | **36.39**   | 33.40      |
| &emsp;- Other              | **37.97**   | 36.00      |
| &emsp;- Social-Science     | **41.66**   | 39.69      |
| **[MMLU-Redux](https://github.com/yuchenlin/ZeroEval)**           | **53.78**| 51.94       |
| **[GSM8K](https://github.com/yuchenlin/ZeroEval)**                | **54.74**| 51.63       |
| **[Truthful QA](https://github.com/EleutherAI/lm-evaluation-harness)**          | **45.43**| 43.72     |
| **[CRUX](https://github.com/yuchenlin/ZeroEval)**          | **21.25**| 20.75     |
| **Chinese-Reason-Bench** | **3.97** | 3.44        |

 The aggregation method for the groups in TMMLUPlus is macro average, following the practice in the official implementation.

### **Alignment Benchmark**

| Metric                   | Kyara    | Gemma-2-2b-it |
|--------------------------|----------|-------------|
| **[AlpacaEval-LC](https://github.com/tatsu-lab/alpaca_eval)**        | **41.15**| 32.37       |
| **[AlpacaEval](https://github.com/tatsu-lab/alpaca_eval)**           | **43.95**| 32.94       |
| **[MT-Bench-TW](https://huggingface.co/datasets/MediaTek-Research/TCEval-v2)**          | **6.94** | 6.59        |
| **[MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench)**             | 7.92     | **8.17**    |

## Feature

Compared to Gemma-2-2b-it, we have introduced the following additional features:

### System Prompt

Kyara supports System Prompt, as the following example:

#### Input

```
System: ä½ æ˜¯ä¸€å€‹æ„›è³£å¼„çŸ¥è­˜çš„äººï¼Œç¸½æ˜¯å–œæ­¡å¼•ç”¨å…¸æ•…ï¼ŒæŠŠç°¡å–®çš„äº‹æƒ…èªªå¾—éå¸¸è¤‡é›œã€‚
User: ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ
```

#### Output

```
å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼Œç°¡ç¨±LLMï¼‰æ˜¯ä¸€ç¨®åŸºæ–¼æ·±åº¦å­¸ç¿’æŠ€è¡“çš„è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ¨¡å‹ã€‚é€™äº›æ¨¡å‹é€šéå¤§é‡æ–‡æœ¬æ•¸æ“šé€²è¡Œè¨“ç·´ï¼Œä»¥å­¸ç¿’å’Œç”Ÿæˆäººé¡èªè¨€ã€‚å…·é«”ä¾†èªªï¼Œå®ƒå€‘å±¬æ–¼ä¸€ç¨®è¢«ç¨±ç‚ºã€Œè®Šæ›å™¨ã€ï¼ˆTransformerï¼‰çš„æ¶æ§‹ã€‚

è®Šæ›å™¨æœ€åˆç”±Vaswaniç­‰äººåœ¨2017å¹´çš„è«–æ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºï¼Œé€™ä¸€æ¶æ§‹å› å…¶åœ¨è™•ç†åºåˆ—æ•¸æ“šä¸Šçš„å“è¶Šæ€§èƒ½è€Œè¿…é€Ÿé¢¨é¡ã€‚è®Šæ›å™¨çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ã€Œè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ã€ï¼ˆSelf-Attention Mechanismï¼‰ï¼Œé€™æ˜¯ä¸€ç¨®èƒ½å¤ æ•æ‰æ–‡æœ¬ä¸­ä¸åŒå–®è©ä¹‹é–“é•·è·é›¢ä¾è³´é—œä¿‚çš„æ–¹æ³•ã€‚

å¤§å‹èªè¨€æ¨¡å‹é€šå¸¸å…·æœ‰ä»¥ä¸‹å¹¾å€‹ç‰¹å¾µï¼š

1. **è¨“ç·´æ•¸æ“šé‡å·¨å¤§**ï¼šé€™äº›æ¨¡å‹é€šå¸¸éœ€è¦æ•¸ç™¾GBç”šè‡³TBç´šçš„è¨“ç·´æ•¸æ“šï¼Œä»¥æ•æ‰èªè¨€çš„è±å¯Œæ€§å’Œå¤šæ¨£æ€§ã€‚

2. **æ¨¡å‹è¦æ¨¡é¾å¤§**ï¼šé€™æ„å‘³è‘—æ¨¡å‹åŒ…å«å¤§é‡åƒæ•¸ï¼Œå¸¸è¦‹çš„è¦æ¨¡åŒ…æ‹¬å¹¾åå„„åˆ°æ•¸è¬å„„å€‹åƒæ•¸ã€‚é€™äº›åƒæ•¸çš„æ•¸é‡æ±ºå®šäº†æ¨¡å‹çš„è¤‡é›œåº¦å’Œç”Ÿæˆæ–‡æœ¬çš„è³ªé‡ã€‚

3. **å¤šèªè¨€èƒ½åŠ›**ï¼šç¾ä»£çš„LLMå¦‚GPT-3ã€BERTç­‰ï¼Œä¸åƒ…èƒ½å¤ è™•ç†å¤šç¨®èªè¨€ï¼Œé‚„èƒ½å¤ åœ¨ä¸åŒèªè¨€ä¹‹é–“è½‰æ›å’Œç†è§£ä¸Šä¸‹æ–‡ã€‚

4. **æ‡‰ç”¨å»£æ³›**ï¼šé€™äº›æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æã€å•ç­”ç³»çµ±ã€æ©Ÿå™¨ç¿»è­¯ã€æ‘˜è¦ç”Ÿæˆç­‰å¤šå€‹NLPä»»å‹™ä¸­éƒ½æœ‰å»£æ³›æ‡‰ç”¨ã€‚

ä¾‹å¦‚ï¼ŒOpenAIçš„GPT-3åŒ…å«1750å„„å€‹åƒæ•¸ï¼Œé€™ä½¿å…¶æˆç‚ºä¸€å€‹æ¥µå…¶å¼·å¤§çš„èªè¨€ç”Ÿæˆæ¨¡å‹ã€‚BERTå‰‡ä»¥å…¶é›™å‘ç·¨ç¢¼å™¨å’Œæ©ç¢¼èªè¨€å»ºæ¨¡æŠ€è¡“ï¼Œåœ¨å¤šç¨®ä¸‹æ¸¸ä»»å‹™ä¸­å–å¾—äº†å“è¶Šçš„æˆç¸¾ã€‚

ç¸½çµä¾†èªªï¼Œå¤§å‹èªè¨€æ¨¡å‹é€šéå¤§é‡æ•¸æ“šè¨“ç·´ï¼Œåˆ©ç”¨è¤‡é›œçš„æ·±åº¦å­¸ç¿’æ¶æ§‹ï¼Œèƒ½å¤ ç”Ÿæˆé«˜è³ªé‡çš„è‡ªç„¶èªè¨€æ–‡æœ¬ï¼Œå¾è€Œåœ¨å„ç¨®NLPæ‡‰ç”¨ä¸­ç™¼æ®é‡è¦ä½œç”¨ã€‚
```

#### Input

```
System: ä½ æ˜¯ä¸€åå°å­¸è€å¸«ï¼Œæœƒç”¨äº”æ­²å°å­©ä¹Ÿèƒ½æ‡‚çš„å£å»å›ç­”ä½¿ç”¨è€…çš„å•é¡Œ
User: ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ
```

#### Output

```
å¥½çš„ï¼æˆ‘ä¾†ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä¸€ä¸‹ã€Œå¤§å‹èªè¨€æ¨¡å‹ã€é€™å€‹æ¦‚å¿µã€‚

æƒ³åƒä¸€ä¸‹ï¼Œä½ æœ‰ä¸€æœ¬éå¸¸å¤§ä¸”è©³ç›¡çš„æ›¸ï¼Œè£¡é¢å¯«äº†å¾ˆå¤šå¾ˆå¤šçš„æ•…äº‹ã€çŸ¥è­˜å’Œè³‡è¨Šã€‚é€™äº›æ•…äº‹å’ŒçŸ¥è­˜å°±åƒæ˜¯å·¨å¤§çš„èªè¨€å€‰åº«ï¼Œè£¡é¢æœ‰å„ç¨®å„æ¨£çš„è©èªå’Œå¥å‹ã€‚

å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼‰ï¼Œç°¡å–®ä¾†èªªï¼Œå°±æ˜¯ä¸€ç¨®éå¸¸è°æ˜çš„æ©Ÿå™¨äººï¼Œå®ƒå­¸ç¿’äº†é€™æœ¬æ›¸è£¡çš„å¤§é‡æ–‡å­—ï¼Œç„¶å¾Œå­¸æœƒäº†å¦‚ä½•ç”¨é€™äº›æ–‡å­—ä¾†å›ç­”å•é¡Œã€å¯«æ•…äº‹ã€è§£é‡‹é“ç†ç­‰ç­‰ã€‚

é€™å€‹æ©Ÿå™¨äººçš„ç‰¹åˆ¥ä¹‹è™•åœ¨æ–¼å®ƒã€Œçœ‹éã€äº†å¾ˆå¤šå¾ˆå¤šæ±è¥¿ï¼Œæ‰€ä»¥å®ƒå¯ä»¥ç†è§£å¾ˆå¤šäº‹æƒ…ï¼Œä¸¦ç”¨å¾ˆæµæš¢çš„èªè¨€çµ¦ä½ è§£é‡‹ã€‚ä½†è¦è¨˜ä½ï¼Œå®ƒé‚„æ˜¯æ©Ÿå™¨ï¼Œä¸æ˜¯çœŸçš„äººï¼Œæ‰€ä»¥æœ‰æ™‚å®ƒçš„å›ç­”å¯èƒ½é‚„ä¸å¤ å®Œç¾ã€‚

èˆ‰å€‹ä¾‹å­ï¼Œå¦‚æœæˆ‘å€‘å•å®ƒã€Œä»€éº¼æ˜¯å¤ªé™½ï¼Ÿã€ï¼Œå®ƒæœƒå¾å®ƒå­¸åˆ°çš„çŸ¥è­˜ä¸­æ‰¾ç­”æ¡ˆï¼Œæ¯”å¦‚èªªï¼šã€Œå¤ªé™½æ˜¯æˆ‘å€‘å¤ªé™½ç³»ä¸­å¿ƒçš„ä¸€é¡†é»ƒçŸ®æ˜Ÿï¼Œå®ƒçµ¦åœ°çƒæä¾›å…‰å’Œç†±ï¼Œè®“æ¤ç‰©èƒ½å¤ åšå…‰åˆä½œç”¨ã€‚ã€

å¸Œæœ›é€™æ¨£çš„è§£é‡‹èƒ½è®“ä½ æ˜ç™½ï¼æœ‰å…¶ä»–å•é¡Œä¹Ÿå¯ä»¥éš¨æ™‚å•æˆ‘ã€‚
```

### Retrieval Augmented Generation (Experimental)

Benefiting from Kyara's training method, we incorporated RAG-related content during the SFT phase. You can refer to the following examples to construct task templates:

#### Input

```
# åƒè€ƒæ–‡ä»¶
<reference>
<document>
æ–‡ä»¶ID:id_816fcfd8
æ–‡ç« æ¨™é¡Œ:å£å‘¼å¸
æ–‡ç« å…§æ–‡:
**å£å‘¼å¸**æŒ‡çš„æ˜¯ç”¨å˜´å‘¼å¸çš„è¡Œç‚ºï¼Œé€™é€šå¸¸æ˜¯ç”±æ–¼é¼»å­å‘¼å¸å—é˜»è€Œå¼•èµ·ï¼Œé¼»å­æ˜¯äººé«”çš„å…ˆå¤©å‘¼å¸å™¨å®˜ã€‚æ…¢æ€§å£å‘¼å¸å¯èƒ½èˆ‡æŸäº›ç–¾ç—…æœ‰é—œã€‚
## åƒè€ƒæ–‡ç»
</document><document>
æ–‡ä»¶ID:id_6c0f7501
æ–‡ç« æ¨™é¡Œ:å£è§’ç‚
æ–‡ç« å…§æ–‡:
**å£è§’ç‚**ï¼ˆè‹±èªï¼š**Angular cheilitis or Angular Stomatitisï¼Œ perlÃ¨che**ï¼‰ï¼Œæˆ–ç¨±çˆ›å˜´è§’ï¼Œç‚ºç™¼ç”Ÿåœ¨å˜´å”‡ä¸€å´æˆ–å…©å´è§’è½éƒ¨ä½çš„ç‚ç—‡ï¼Œé€šå¸¸ç‚ºå…©å´åŒæ™‚ç™¼ç‚ã€‚æ­¤ç—‡æ˜¯å”‡ç‚ï¼ˆcheilitisï¼‰çš„ä¸€ç¨®å½¢å¼ï¼Œç™¼ç‚éƒ¨ä½çš®è†šé€šå¸¸æœƒç´…è…«ã€è„«çš®åŠçµç—‚ï¼Œä¹Ÿå¯èƒ½æœƒé€ æˆç™¼ç™¢æˆ–ç–¼ç—›ï¼Œç—‡ç‹€å¯æŒçºŒæ•¸å¤©ç”šè‡³å¯é”æ•¸å¹´ä¹‹ä¹…ã€‚
å£è§’ç‚å¯èƒ½å› æ„ŸæŸ“ã€å‰Œæ¿€æˆ–æ˜¯éæ•è€Œå¼•èµ·ã€‚æ„ŸæŸ“æºåŒ…å«å¦‚ç™½è‰²å¿µç èŒç­‰çœŸèŒä»¥åŠå¦‚é‡‘é»ƒè‰²è‘¡è„çƒèŒç­‰ç´°èŒã€‚å‰Œæ¿€æºåŒ…å«é…å¸¶ä¸é©ç•¶çš„å‡ç‰™ã€èˆ”å˜´å”‡ã€ç”¨å˜´å·´å‘¼å¸å°è‡´çš„å˜´éƒ¨ä¹¾ç‡¥ã€æ—¥æ›¬ã€å˜´éƒ¨éåº¦é–‰åˆã€æŠ½è¸ï¼Œä»¥åŠè¼•å¾®å‰µå‚·ã€‚éæ•æºå‰‡åŒ…å«ç‰™è†ã€åŒ–å¦å“åŠé£Ÿå“ç­‰ç‰©è³ªã€‚å…¶ä»–ç—…å› å¯èƒ½åŒ…å«ç‡Ÿé¤Šä¸è‰¯æˆ–å…ç–«åŠŸèƒ½ä¸è‰¯ ã€‚æœƒç™¼ç”Ÿæ­¤ç—‡é€šå¸¸æ˜¯å¤šé‡å› ç´ ä½œç”¨çš„çµæœï¼Œå°æ‚£è€…é€²è¡Œæ„ŸæŸ“åŠçš®è†šéæ•æºæ¸¬è©¦å°‡æœ‰åŠ©æ–¼è¨ºæ–·è‚‡å› ã€‚
å£è§’ç‚çš„æ²»ç™‚ä¸€èˆ¬è€Œè¨€æ˜¯åœ¨æ‰¾å‡ºè‚‡å› å¾Œä½¿ç”¨é©ç•¶çš„é˜²è­·éœœï¼ˆè­·è†šè†ï¼‰ï¼Œä¹Ÿå¸¸å˜—è©¦ä½¿ç”¨æŠ—é»´èŒåŠæŠ—ç´°èŒè»Ÿè†åŠ ä»¥æ²»ç™‚ã€‚æ­¤ç—‡å¯èªªæ˜¯ç›¸ç•¶å¸¸è¦‹çš„ç–¾ç—…ï¼Œæ“šä¼°è¨ˆåœ¨ç¾åœ‹ç´„æœ‰ 0.7% çš„äººå—åˆ°æ­¤ç—‡å½±éŸ¿ ã€‚å£è§’ç‚å¥½ç™¼æ–¼ 30 è‡³ 60 æ­²é–“çš„äººï¼Œä½†åœ¨å­©ç«¥èº«ä¸Šä¹Ÿç›¸å°å¸¸è¦‹ã€‚åœ¨é–‹ç™¼ä¸­åœ‹å®¶ï¼Œç¼ºä¹éµè³ªåŠç¶­ä»–å‘½æ˜¯æ­¤ç—‡å¸¸è¦‹çš„è‚‡å› ã€‚é•·æœŸè™•æ–¼æ½®æ¿•åè€Œæœƒä½¿å£è§’ç‚æ›´åš´é‡ï¼Œå˜´è§’æ›´ä¹¾ç‡¥ï¼Œä¸”è„«çš®æ„ˆåš´é‡ï¼Œé©ç•¶ä¿æŒä¹¾ç‡¥æ‰èƒ½è®“ç´°èŒå£æ­»ã€‚
## ç—…å› 
å› ç—…å› ä¸åŒè€Œåˆ†ç‚ºç‡Ÿé¤Šä¸è‰¯æ€§å£è§’ç‚ã€çƒèŒæ€§å£è§’ç‚ã€çœŸèŒæ€§å£è§’ç‚ã€‚
ç‡Ÿé¤Šä¸è‰¯æ€§å£è§’ç‚å¤šç‚ºç¼ºä¹ç¶­ç”Ÿç´  B æ—ï¼Œå¤šç‚º B2 æ ¸é»ƒç´ æˆ– B12 éˆ·èƒºç´ ï¼Œé€ æˆçš„å˜´è§’è²§è¡€ã€‚ä»¥åŠç¼ºéµã€ç¼ºé‹…ã€‚
çƒèŒæ€§å£è§’ç‚å’ŒçœŸèŒæ€§å£è§’ç‚ç‚ºç´°èŒæˆ–çœŸèŒæ„ŸæŸ“ï¼Œç´°èŒæˆ–çœŸèŒè¢«å¸¶åˆ°å˜´è§’å¾Œåœ¨æ¿•æ½¤çš„ç’°å¢ƒä¸‹å®¹æ˜“å½¢æˆç‚ç—‡ï¼Œå› æ­¤æ‡‰ä½¿å˜´è§’å„˜é‡ä¹¾ç‡¥ï¼Œä½¿ç´°èŒä¸æ˜“å­˜æ´»ã€‚åŸå› å¯èƒ½ç‚ºéæ›¬æˆ–éå¹²ï¼ˆèˆ”å˜´è§’ï¼‰ï¼Œæ©Ÿæ¢°åŸå› å¦‚é–‰åˆä¸ç•¶ï¼Œå‡ç‰™ä¸é©æˆ–è€å¹´æ‰ç‰™éåº¦é–‰åˆï¼ŒåŠå˜´è§’æµæ¶é€ æˆçš„ã€‚
## ç—‡ç‹€
å–®å´æˆ–å…©å´å£è§’æ¿•ç™½è‰²ï¼Œç´…è…«ï¼Œæ½°çˆ›ï¼Œçµç—‚ï¼Œæœ‰ç‡’ç¼æ„Ÿã€‚å£è§’ç™¼ç·Šï¼Œé‹å‹•é–‹è£‚ã€‚
## è¨ºæ–·
ç‡Ÿé¤Šä¸è‰¯æ€§å£è§’ç‚å¯èƒ½æœƒæœ‰èˆŒï¼Œå£è…”ï¼Œé™°éƒ¨é»è†œç­‰å…¨èº«æ€§çš„ç›¸æ‡‰ç—‡ç‹€ã€‚æˆ–ä¼´æœ‰è†¿ç–±ï¼Œå¤šèˆ‡åŒ–è†¿çƒèŒæ„ŸæŸ“æœ‰é—œã€‚
## æ²»ç™‚
ç‡Ÿé¤Šä¸è‰¯æ€§å£è§’ç‚ç›´æ¥æ–½èˆ‡ B æ—ç¶­ç”Ÿç´ å³å¯ï¼ŒçƒèŒæ€§å£è§’ç‚è—¥ç‰©ä¸€èˆ¬è™•æ–¹æŠ—ç”Ÿç´ ï¼Œè€ŒçœŸèŒæ€§å£è§’ç‚è—¥ç‰©å‰‡è™•æ–¹æŠ—çœŸèŒè—¥ç‰©ã€‚
## åƒè€ƒè³‡æ–™
</document><document>
æ–‡ä»¶ID:id_a214252f
æ–‡ç« æ¨™é¡Œ:æ‰“å‘¼
æ–‡ç« å…§æ–‡:
**é¼»é¼¾**æ˜¯å‘¼å¸ç³»çµ±çš„çµæ§‹éœ‡å‹•è€Œç”¢ç”Ÿçš„è²éŸ³ï¼ŒåŸå› æ˜¯ç¡è¦ºæ™‚å‘¼å¸è¢«é˜»æ“‹ã€‚åœ¨ä¸€äº›æƒ…æ³ä¸‹è²éŸ³è¼ƒè¼•ï¼Œä½†ä¸€èˆ¬æƒ…æ³ä¸‹éƒ½æ˜¯å˜ˆåµåŠç…©äººçš„ã€‚é¼»é¼¾åŒæ™‚å¯èƒ½æ˜¯ç¡çœ çª’æ¯ç—‡çš„ç¬¬ä¸€å€‹è­¦è™Ÿã€‚ç ”ç©¶æŒ‡å‡ºé¼»é¼¾æ˜¯ç¡çœ ä¸è¶³çš„ä¸€é …å› ç´ ã€‚
## åç¨±
è¡¨ç¤ºç™¼å‡ºé¼¾è²çš„æ„æ€ï¼Œå¯ç”¨ã€Œæ‰“é¼¾ã€ï¼Œæˆ–ä¿—ã€Œæ‰“å‘¼ã€ã€Œæ‰“å‘¼åš•ã€ã€‚ã€Œé¼¾ã€å…¶å­—å…¶éŸ³ï¼Œè‡³å°‘åœ¨æ±æ¼¢ä»¥å‰å°±è¨“çˆ²é¼¾è²çš„æ„æ€ã€‚
## æˆå› 
é¼»é¼¾é€šå¸¸ç”±æ–¼æ‡¸é›å‚å’Œè»Ÿé¡é¬†å¼›è€Œå¼•èµ·ï¼Œé¬†å¼›çš„è»Ÿçµ„ç¹”æœƒä»¤åˆ°æ°£ç®¡é˜»å¡æˆ–ä¸æš¢é€šï¼Œå°è‡´ä¸è¦å‰‡çš„æ°£æµå’ŒæŒ¯å‹•ã€‚ä»¥ä¸‹çš„æƒ…æ³éƒ½å¯èƒ½æ˜¯é¼»é¼¾çš„æˆå› ï¼š
* ç”Ÿæ´»ç¿’æ…£ï¼Œå¦‚å¸è¸ã€é…—é…’æˆ–æ¿«è—¥
* åš¥å–‰ç„¡åŠ›ï¼Œå°è‡´ç¡çœ æœŸé–“åš¥å–‰é—œé–‰
* é¡ä½ä¸å°é½Šï¼Œé€šå¸¸æ˜¯ç”±æ–¼è‚Œè‚‰ç·Šå¼µæ‰€è‡´
* è‚¥èƒ–ç—‡ï¼Œéå¤šè„‚è‚ªç©èšæ–¼åš¥å–‰é™„è¿‘
* é¼»è…”é˜»å¡
* é˜»å¡æ€§ç¡çœ å‘¼å¸æš«åœ
* ç¡çœ ä¸è¶³
* ä»°ç¡
* ä»¥å£å‘¼å¸
## åƒè€ƒè³‡æ–™
</document><document>
æ–‡ä»¶ID:id_964dde34
æ–‡ç« æ¨™é¡Œ:äººå·¥å‘¼å¸
æ–‡ç« å…§æ–‡:
**å£å°å£äººå·¥å‘¼å¸**æ˜¯äººå·¥å‘¼å¸çš„ä¸€ç¨®å½¢å¼ï¼Œæ˜¯äººå€‘å”åŠ©æˆ–åˆºæ¿€ä»–äººå‘¼å¸çš„è¡Œç‚ºï¼Œäººå€‘åœ¨äººå·¥å‘¼å¸æ™‚è¦ç”¨å˜´æŠµä½è¢«æ•‘æ´è€…çš„å˜´ï¼Œç„¶å¾Œå°‡ç©ºæ°£å¹å…¥è¢«æ•‘æ´è€…çš„è‚ºéƒ¨ã€‚é€šå¸¸ç„¡æ³•è‡ªä¸»å‘¼å¸çš„äººéœ€è¦äººå·¥å‘¼å¸ã€‚18 ä¸–ç´€æœ«ï¼Œè‹±åœ‹çš„é†«ç”Ÿé–‹å§‹ç©æ¥µæ™®åŠäººå·¥å‘¼å¸ã€‚
## åƒè€ƒæ–‡ç»
</document>
</reference>

---

# ä»»å‹™èªªæ˜

ä½ æ˜¯ä¸€åç ”ç©¶å“¡ï¼Œè«‹è©³ç´°é–±è®€ä¸Šè¿°åƒè€ƒæ–‡ä»¶ï¼Œä»¥é€šä¿—æ˜“æ‡‚çš„ç¹é«”ä¸­æ–‡å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
åœ¨å›ç­”å•é¡Œæ™‚ï¼Œè«‹æ³¨æ„ä»¥ä¸‹å¹¾é»ï¼š
1. è«‹é©ç•¶çš„æ´å¼•åƒè€ƒæ–‡ä»¶çš„è³‡è¨Šï¼Œä¾†ä½è­‰ä½ çš„å›ç­”è«–è¿°ï¼Œæ´å¼•æ–¹å¼å¦‚æ’°å¯«è«–æ–‡æ™‚åœ¨å¼•ç”¨åƒè€ƒè³‡æ–™ä¸€èˆ¬ï¼Œå¯åƒè€ƒä¾‹å­ï¼š

å‡è¨­æˆ‘å€‘æœ‰æ–‡ç« 1 id: dq945509 èˆ‡æ–‡ç« 2 id: 6a178c5ï¼Œå¦‚æœæˆ‘å€‘æƒ³é€²è¡Œå¼•è¿°æ™‚è¦æ ¼å¦‚ä¸‹ï¼šé€™æ˜¯ä¸€æ®µäº‹å¯¦è«–è¿°ã€dq945509ã€‘ï¼Œè€Œé€™æ˜¯å¦ä¸€æ®µäº‹å¯¦è«–è¿°ã€6a178c5ã€‘ã€‚

2. **è«‹æ³¨æ„ï¼Œå¦‚æœç™¼ç¾åƒè€ƒæ–‡ä»¶èˆ‡ä½¿ç”¨è€…çš„å•é¡Œç„¡é—œï¼Œæˆ–è€…ç„¡åŠ©æ–¼å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œè«‹ç›´æ¥å›ç­”ï¼šã€Œåƒè€ƒæ–‡ä»¶ç¼ºä¹èˆ‡å•é¡Œç›¸é—œè³‡è¨Šã€ï¼Œä¸è¦æœæ’°ç„¡é—œçš„å›ç­”ã€‚**

# ä½¿ç”¨è€…å•é¡Œ
ç‚ºä»€éº¼èªªç”¨å˜´å‘¼å¸ä¸å¥åº·ï¼Ÿ
```

#### Output

```
ç”¨å˜´å‘¼å¸ä¸å¥åº·çš„åŸå› ä¸»è¦æœ‰ä»¥ä¸‹å¹¾é»ï¼š

1. **ä¹¾ç‡¥å’Œåˆºæ¿€**ï¼šç”¨å˜´å‘¼å¸æœƒä½¿å£è…”å…§éƒ¨å’ŒèˆŒé ­è®Šå¾—éå¸¸ä¹¾ç‡¥ï¼Œå®¹æ˜“å¼•èµ·å£è…”é»è†œçš„åˆºæ¿€å’Œç‚ç—‡ã€‚ä¾‹å¦‚ï¼Œå£è§’ç‚å°±æ˜¯å› ç‚ºå˜´éƒ¨ä¹¾ç‡¥ã€ä¸é©ç•¶çš„ç’°å¢ƒï¼ˆå¦‚ç”¨å˜´å·´å‘¼å¸ï¼‰å°è‡´çš„ç‚ç—‡ã€id_6c0f7501ã€‘ã€‚

2. **æ„ŸæŸ“é¢¨éšªå¢åŠ **ï¼šç”¨å˜´å‘¼å¸æœƒä½¿å£è…”å…§çš„ç´°èŒå’Œç—…æ¯’æ›´å®¹æ˜“é€²å…¥å‘¼å¸é“ï¼Œå¢åŠ æ„ŸæŸ“çš„é¢¨éšªã€‚ä¾‹å¦‚ï¼Œå£è§’ç‚çš„ç—…å› ä¹‹ä¸€å°±æ˜¯å› ç‚ºå£è…”å…§çš„ç´°èŒæˆ–çœŸèŒæ„ŸæŸ“ã€id_6c0f7501ã€‘ã€‚

3. **ç¡çœ è³ªé‡ä¸‹é™**ï¼šç”¨å˜´å‘¼å¸æœƒå°è‡´æ‰“å‘¼ï¼Œé€™æ˜¯å› ç‚ºæ‡¸é›å‚å’Œè»Ÿé¡é¬†å¼›ï¼Œå°è‡´æ°£ç®¡é˜»å¡æˆ–ä¸æš¢é€šï¼Œå¾è€Œå¼•èµ·æ‰“é¼¾ï¼Œé€²è€Œå½±éŸ¿ç¡çœ è³ªé‡ã€id_a214252fã€‘ã€‚

4. **æ…¢æ€§å•é¡Œ**ï¼šé•·æœŸç”¨å˜´å‘¼å¸å¯èƒ½å¼•ç™¼æ…¢æ€§å£å‘¼å¸ï¼Œé€™æœƒå°å¥åº·ç”¢ç”Ÿå¤šæ–¹é¢çš„å½±éŸ¿ã€id_816fcfd8ã€‘ã€‚

å› æ­¤ï¼Œç‚ºäº†ç¶­è­·å‘¼å¸é“å¥åº·ï¼Œæ‡‰å„˜é‡é€šéé¼»å­é€²è¡Œå‘¼å¸ï¼Œä»¥æ¸›å°‘ä¸Šè¿°å•é¡Œçš„ç™¼ç”Ÿã€‚
```

Please refer to [Kyara-RAG](https://github.com/zake7749/kyara-rag) for more details.

## Method

The following sections provide a brief summary of Kyara's implementation strategy.

### Dataset Summary

We have collected a total of 2.6M conversations, approximately 3.54 billion tokens. The following provides an overview of the language distribution and conversation rounds.

* Languageï¼š

<img src="https://i.imgur.com/KvVjti4.png" alt="language-distribution" width="500"/>

* Conversation Roundsï¼š

<img src="https://i.imgur.com/dekAnU0.png" alt="conv-round-distribution" width="500"/>

### Dataset Construction

#### Base Dataset: Knowledge Injection with Retrieval Augmentation

We developed a knowledge search system using open Chinese knowledge corpora, integrated with [QDrant](https://qdrant.tech/). To construct Supervised Fine-Tuning(SFT) pairs, we followed this process:

1. Sample documents from the knowledge base and generate knowledge-intensive questions that users might ask based on these texts.
2. (Optional) Increase instruction complexity using [Evol-Instruct](https://arxiv.org/pdf/2304.12244).
3. Apply query expansion on the generated instructions to retrieve additional Top K documents and individually assess their relevance:
   * For relevant documents, use an LLM to summarize key information related to the question.
   * For irrelevant documents, ignore them.
4. Let the LLM generate a detailed and comprehensive response according to the original document and K supplementary references.

Besides, we would also aks LLM to generate an user prompt for high quality documents, and pair the (generated prompt, original document) as a SFT example.

##### Chinese Math Dataset

* Dataset: [zake7749/kyara-chinese-math-sft-s0-30K](https://huggingface.co/datasets/zake7749/kyara-chinese-math-sft-s0-30K)

While the aforementioned strategy can generate a wide range of knowledge-based texts, it primarily falls within the scope of information-seeking tasks and is not very effective in constructing mathematical and reasoning-related content. To address this, we generated 50,000 math problems based on [PersonaHub](https://huggingface.co/datasets/proj-persona/PersonaHub). We then used `Gemini-1.5-Flash` to filter out data with obvious errors in calculation and reasoning, thereby creating [kyara-chinese-math-sft-s0-30K](https://huggingface.co/datasets/zake7749/kyara-chinese-math-sft-s0-30K).

#### High Quality Dataset: Model Refinement 

After completing supervised learning using the base dataset, we will fine-tune the LLM again on a high-quality subset, primarily to address the following three issues:

1. Some responses in the Base Dataset were generated from small model, which sometimes performed poorly in following instructions.
2. We used various LLMs in the previous step to introduce knowledge diversity and language adaptability. However, we discovered subtle differences in response templates and reasoning approaches between different LLMs, leading to occasional instability in the trained Chat Model. Therefore, we would like to introduced a high-quality small dataset, using a single strong LLM to generate QA Pairs.
3. The Base Dataset includes some Q&A Pairs composed of generated queries and original document. While these data are rich in knowledge, they are relatively weak in terms of instruction following.

To balance data diversity and quality, we adopted a strategy similar to [InsTag](https://arxiv.org/abs/2308.07074) to classify the data. We then used [ArmoRM](https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1) and an LLM Judge to evaluate data quality, finally extracting the best training data from each category to create the Stage 1 Dataset of about 200K, which was used to fine-tune the Kyara-SFT Model again.

### Preference Learning

We introduced Preference Learning in Kyara, which allows the model's responses to better align with human preferences while enhancing programming skills and mathematical reasoning abilities.

Kyaraâ€™s preference learning strategy utilizes Direct Preference Optimization (DPO), integrating two custom-built Chinese datasets alongside two English datasets.

* [argilla/ultrafeedback-binarized-preferences](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences)
* [xinlai/Math-Step-DPO-10K](https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K)

Here, we summarize the construction strategy of the Chinese datasets.

#### Chinese DPO

##### [SPIN/SPPO](https://github.com/uclaml/SPIN)

We followed the original design, using Kyara-SFT to generate a set of contrastive data for the High Quality Dataset.

##### RLAIF

è³‡æ–™é›†ï¼š[zake7749/kyara-chinese-preference-dpo-s0-30K](https://huggingface.co/datasets/zake7749/kyara-chinese-preference-dpo-s0-30K)

We extracted Chinese Prompts from `Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese`, `hfl/stem_zh_instruction`, and `FreedomIntelligence/Evol-Instruct-Chinese-GPT4`, and distributed the same prompt to four different LLMs. The competitors include:

* GPT-4o
* GPT-4-0618
* ChatGPT-3.5-0513
* Claude-Sonnet-3.5
* Yi-Large
* Mixtral 8x22B
* Gemini-Flash
* Qwen2-72B-Instruct
* DeepSeek V2

After response generation, we ask the LLMs to judge which one is better, using the following prompt:

```
**[Task]**

Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness.  
1. First, independently solve the user question step-by-step.  
2. Then, compare both assistantsâ€™ answers with your answer. Identify and correct any mistakes.  
3. Do not allow the length of the responses to influence your evaluation.  
4. Be as objective as possible.

After providing your explanation, output your final verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]" if assistant B is better, and "[[C]]" for a tie or if both A and B are bad.  

If the answers from A and B are very similar in terms of correctness, helpfulness, and relevance, meaning there is no "obvious" winner, judge it as a tie and output [[C]].

**[User Question]**  
{prompt}

---

**[Assistant Aâ€™s Answer]**  
{answer}

---

**[Assistant Bâ€™s Answer]**  
{prediction}

---
```

Finally, all four datasets were combined for DPO training.

## Limitation

Like most models, Kyara still suffers from hallucinations, and possibly due to the nature of the data, it particularly enjoys quoting references when answering questions. For example, if you ask the model to explain a line from "Legend of Mortal":

```
å¦‚ä½•è§£é‡‹ã€Œä¿ è€…ï¼Œä¸å—ååˆ©æ‰€ç‰½ï¼Œä½†æ±‚ä¿¯ä»°ç„¡æ„§ã€‚æ­£é‚ªå­˜ä¹åœ¨æˆ‘ï¼Œè²åç•™å•è«¸å›ã€‚éæˆ‘æ¡€å‚²ä¸é¦´ï¼Œä¹ƒå› å”é–€è–ªç«ï¼Œè‡ªå¤å¦‚ä»Šï¼ã€
```

The model would respond that this quote is from The "Legend of the Condor Heroes."

```
é€™æ®µè©±å‡ºè‡ªã€Šå°„éµ°è‹±é›„å‚³ã€‹ä¸­çš„å”é–€å…§éƒ¨ç¶“å…¸ï¼Œç”±éƒ­é–å’Œé»ƒè“‰åœ¨å”é–€å¤§æ®¿ä¸Šå®£è®€ã€‚é€™æ®µè©±ä¸»è¦è¡¨é”äº†å”é–€é–€äººå°æ–¼ä¿ ç¾©ç²¾ç¥å’Œå…§åœ¨ä¿®ç‚ºçš„è¿½æ±‚ï¼Œä»¥åŠä»–å€‘å°å¤–ç•Œçš„ååˆ©å’Œè²åçš„ä¸å±‘ã€‚ä»¥ä¸‹æ˜¯è©³ç´°è§£é‡‹ï¼š

1. **ã€Œä¿ è€…ï¼Œä¸å—ååˆ©æ‰€ç‰½ã€**ï¼š
   - ä¿ è€…ï¼ˆä¿ å£«ï¼‰æ˜¯æŒ‡é‚£äº›ä¸ç‚ºååˆ©æ‰€å‹•æ–ã€ä¸è¿½æ±‚ä¸–ä¿—æ¦®è¯å¯Œè²´çš„äººã€‚é€™å¥è©±å¼·èª¿äº†ä»–å€‘è¿½æ±‚çš„æ˜¯å…§åœ¨çš„ä¿®ç‚ºå’Œé“å¾·ï¼Œè€Œéå¤–åœ¨çš„åè²å’Œåˆ©ç›Šã€‚

2. **ã€Œä½†æ±‚ä¿¯ä»°ç„¡æ„§ã€**ï¼š
   - ä¿¯ä»°ç„¡æ„§æ„æŒ‡åœ¨è¡Œå‹•å’Œè¨€è¡Œä¸Šç„¡æ„§æ–¼å¿ƒï¼Œæ²’æœ‰ä»»ä½•è‡ªç§çš„å‹•æ©Ÿã€‚é€™æ„å‘³è‘—ä»–å€‘åªè¿½æ±‚å…§å¿ƒçš„å…‰æ˜å’Œæ­£ç¾©ï¼Œä¸ç‚ºå¤–ç•Œçš„è©•åƒ¹æ‰€å‹•æ–ã€‚

3. **ã€Œæ­£é‚ªå­˜ä¹åœ¨æˆ‘ï¼Œè²åç•™å•è«¸å›ã€**ï¼š
   - æ­£é‚ªå­˜ä¹åœ¨æˆ‘ï¼Œæ„æŒ‡ä»–å€‘å…§å¿ƒæœ‰æ­£é‚ªä¹‹åˆ†ï¼Œä½†ä»–å€‘æœƒåœ¨è¡Œå‹•ä¸­ä¿æŒæ­£ç¾©ï¼Œä¸è®“é‚ªæƒ¡ä¾µæŸ“ã€‚è²åç•™å•è«¸å›ï¼Œæ„å‘³è‘—ä»–å€‘çš„è¡Œç‚ºå’Œå“è¡Œæœƒè®“ä¸–äººè­°è«–å’Œç¨±é“ï¼Œä½†ä»–å€‘ä¸¦ä¸åœ¨æ„å¤–ç•Œçš„è©•åƒ¹ã€‚

4. **ã€Œéæˆ‘æ¡€å‚²ä¸é¦´ï¼Œä¹ƒå› å”é–€è–ªç«ï¼Œè‡ªå¤å¦‚ä»Šï¼ã€**ï¼š
   - æ¡€å‚²ä¸é¦´ï¼ˆæ¡€å‚²ï¼‰æŒ‡çš„æ˜¯æ¡€é©ä¸é¦´ã€æ¡€é©ä¸ç¾ˆï¼Œå½¢å®¹é‚£äº›æ¡€é©ä¸é¦´ã€ä¸è½å¾æ¬Šå¨çš„äººã€‚é€™å¥è©±è§£é‡‹äº†å”é–€é–€äººä¹‹æ‰€ä»¥æ¡€é©ä¸é¦´ï¼Œæ˜¯å› ç‚ºä»–å€‘ç¹¼æ‰¿äº†å”é–€è–ªç«ï¼Œé€™é–€æ´¾è‡ªå¤ä»¥ä¾†å°±ä»¥é«˜å°šçš„é“å¾·å’Œå¼·å¤§çš„æ­¦åŠŸèåï¼Œé–€äººè‡ªç„¶è¦ä¿æŒé€™ç¨®é¢¨ç¯„ã€‚

ç¸½çµä¾†èªªï¼Œé€™æ®µè©±å¼·èª¿äº†å”é–€é–€äººå°ååˆ©å’Œè²åçš„ä¸å±‘ï¼Œä»–å€‘è¿½æ±‚çš„æ˜¯å…§åœ¨çš„ä¿®ç‚ºå’Œé“å¾·ï¼Œä¸ç‚ºå¤–ç•Œçš„è©•åƒ¹æ‰€å‹•æ–ã€‚é€™ä¹Ÿé«”ç¾äº†ä»–å€‘ä¿ ç¾©ç²¾ç¥å’Œå°æ­£ç¾©çš„å …æŒã€‚
```

It is recommended to exercise caution when using language models.
